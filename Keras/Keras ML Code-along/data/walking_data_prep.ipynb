{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Keras: Chest accelerometer data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Files from the download are in a directory called \"activity\" saved in the same place as this notebook\n",
    "import glob\n",
    "data_files = glob.glob(\"activity/*.csv\")\n",
    "\n",
    "# Initialise array for data:\n",
    "#   Rows will be observations\n",
    "#   Columns will be:\n",
    "#   * Time point (sequential count integer)\n",
    "#   * x-, y-, z-directional accelerometer data time series (integer)\n",
    "#   * Activity label (1-7)\n",
    "#   * Person label (0-14)\n",
    "dataset = np.empty((0, 6), dtype = \"float64\")\n",
    "\n",
    "# Add data from each file in turn\n",
    "for i in range(len(data_files)):\n",
    "    \n",
    "    print(\"Reading file\", i+1, \"/\", len(data_files))\n",
    "    \n",
    "    f = data_files[i]\n",
    "    data = np.genfromtxt(f, delimiter=',')\n",
    "    \n",
    "    # Add a column with a label representing the person\n",
    "    # (this doesn't necessarily line up with the file number)\n",
    "    augmented = np.column_stack(\n",
    "        (data, np.array([i]*data.shape[0]))\n",
    "    )\n",
    "    dataset = np.vstack((dataset, augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check how many observations we have\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape data into 3 dimensions:\n",
    "#   0-dimension (\"rows\") is observations (1926896 in total)\n",
    "#   1-dimension (\"columns\") is time series values (260 = 5{seconds}*52{Hz} in total)\n",
    "#   2-dimension (\"leaves\") are as follows (5 in total):\n",
    "#     * 3 directions (x-, y-, z-acceleration)\n",
    "#     * Activity type labels\n",
    "#     * Person labels\n",
    "\n",
    "# We'll chop the time series into 260-length (5 second) sections every 52 points (every 1 second)\n",
    "t = int((dataset.shape[0]-208) / 52)\n",
    "chopped = -np.ones((t, 260, 5), dtype=\"float64\")\n",
    "\n",
    "for k in range(0, t):\n",
    "    start, stop = (52*k, 52*k + 260)\n",
    "    # If the count column's value at \"stop\" is smaller than at \"start\", we've changed person, so discard\n",
    "    # If the activity label column is not all the same, we have more than one activity in that section, so discard\n",
    "    if (dataset[stop, 0] < dataset[start, 0] or not all(dataset[start:stop, 4] == dataset[start, 4])):\n",
    "        continue\n",
    "    # Else copy all but count column to the new data block\n",
    "    chopped[k, :, :] = dataset[start:stop, 1:6]\n",
    "\n",
    "# Remove the extra rows, which will have person label -1\n",
    "chopped = chopped[(chopped[:, 0, 4] != -1), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check shape again\n",
    "chopped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Walking\" corresponds to activity label 4\n",
    "walking = chopped[(chopped[:, 0, 3] == 4), :, :]\n",
    "\n",
    "# Scale each time series individually, because recorded data is not necessarily calibrated\n",
    "from sklearn.preprocessing import scale\n",
    "walking_data = np.apply_along_axis(scale, 1, walking[:, :, 0:3])\n",
    "\n",
    "# The person label is in layer 4, and it's the same in all columns so we just get it from column 0\n",
    "walking_labels = walking[:, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "walking_data.shape\n",
    "walking_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save this information in .npy files\n",
    "np.save(arr=walking_data, file=\"walking_data.npy\")\n",
    "np.save(arr=walking_labels, file=\"walking_labels.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
